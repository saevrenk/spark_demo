{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# remember to change the path/location of file\n",
    "airports = sc.textFile(\"/Users/sae/vs_projects/spark_demo/spark_files/airports.txt.text\")\n",
    "airports.take(9)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/08 13:30:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/08 13:30:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/08 13:30:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)]\n",
    "\n",
    "columns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "# ------Creating data frame Using createDataFrame() function-----\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id    name       city\n",
      "0  1001   Young  Rego Park\n",
      "1  1002   James      Bronx\n",
      "2  1003  Haseeb    Astoria\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sae/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/sae/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+\n",
      "|  id|  name|     city|\n",
      "+----+------+---------+\n",
      "|1001| Young|Rego Park|\n",
      "|1002| James|    Bronx|\n",
      "|1003|Haseeb|  Astoria|\n",
      "+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "student_dict = {\"id\": [1001, 1002, 1003],\"name\": [ \"Young\", \"James\", \"Haseeb\"],\"city\": [ \"Rego Park\", \"Bronx\", \"Astoria\"]}\n",
    "\n",
    "# --- panda dataframe ----\n",
    "pd_df = pd.DataFrame(student_dict)\n",
    "print(pd_df)\n",
    "\n",
    "\n",
    "# ----- SparkSQL dataframe-----\n",
    "sp_df = spark.createDataFrame(pd_df)\n",
    "sp_df.printSchema()\n",
    "sp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MLS: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Bedrooms: integer (nullable = true)\n",
      " |-- Bathrooms: integer (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Price SQ Ft: double (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create df from csv\n",
    "df = spark.read.load(\"spark_files//RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               null|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         null|           1|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               null|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         null|           2|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               null|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         null|       61391|   TX|           null|      null|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         null|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         null|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               null|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         null|           4|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         null|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         null|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               null|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         null|       49346|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         null|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               null|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         null|       49348|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               null|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         null|           3|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               null|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         null|       54354|   AL|           null|      null|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         null|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         null|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         null|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         null|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"spark_files/zipcode.json\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
      "|productCode|         productName|     productLine|productScale|       productVendor|  productDescription|quantityInStock|buyPrice|  MSRP|\n",
      "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
      "|   S10_1678|1969 Harley David...|     Motorcycles|        1:10|     Min Lin Diecast|This replica feat...|           7933|   48.81| 95.70|\n",
      "|   S10_1949|1952 Alpine Renau...|    Classic Cars|        1:10|Classic Metal Cre...|Turnable front wh...|           7305|   98.58|214.30|\n",
      "|   S10_2016|1996 Moto Guzzi 1...|     Motorcycles|        1:10|Highway 66 Mini C...|Official Moto Guz...|           6625|   68.99|118.94|\n",
      "|   S10_4698|2003 Harley-David...|     Motorcycles|        1:10|   Red Start Diecast|Model features, o...|           5582|   91.02|193.66|\n",
      "|   S10_4757| 1972 Alfa Romeo GTA|    Classic Cars|        1:10|Motor City Art Cl...|Features include:...|           3252|   85.68|136.00|\n",
      "|   S10_4962|1962 LanciaA Delt...|    Classic Cars|        1:10| Second Gear Diecast|Features include:...|           6791|  103.42|147.74|\n",
      "|   S12_1099|   1968 Ford Mustang|    Classic Cars|        1:12|Autoart Studio De...|Hood, doors and t...|             68|   95.34|194.57|\n",
      "|   S12_1108|   2001 Ferrari Enzo|    Classic Cars|        1:12| Second Gear Diecast|Turnable front wh...|           3619|   95.59|207.80|\n",
      "|   S12_1666|      1958 Setra Bus|Trucks and Buses|        1:12|Welly Diecast Pro...|Model features 30...|           1579|   77.90|136.67|\n",
      "|   S12_2823|    2002 Suzuki XREO|     Motorcycles|        1:12|Unimax Art Galleries|Official logos an...|           9997|   66.27|150.62|\n",
      "|   S12_3148|  1969 Corvair Monza|    Classic Cars|        1:18|Welly Diecast Pro...|1:18 scale die-ca...|           6906|   89.14|151.08|\n",
      "|   S12_3380|  1968 Dodge Charger|    Classic Cars|        1:12|Welly Diecast Pro...|1:12 scale model ...|           9123|   75.16|117.44|\n",
      "|   S12_3891|    1969 Ford Falcon|    Classic Cars|        1:12| Second Gear Diecast|Turnable front wh...|           1049|   83.05|173.02|\n",
      "|   S12_3990|1970 Plymouth Hem...|    Classic Cars|        1:12| Studio M Art Models|Very detailed 197...|           5663|   31.92| 79.80|\n",
      "|   S12_4473|   1957 Chevy Pickup|Trucks and Buses|        1:12|       Exoto Designs|1:12 scale die-ca...|           6125|   55.70|118.50|\n",
      "|   S12_4675|  1969 Dodge Charger|    Classic Cars|        1:12|Welly Diecast Pro...|Detailed model of...|           7323|   58.73|115.16|\n",
      "|   S18_1097|1940 Ford Pickup ...|Trucks and Buses|        1:18| Studio M Art Models|This model featur...|           2613|   58.33|116.67|\n",
      "|   S18_1129|     1993 Mazda RX-7|    Classic Cars|        1:18|Highway 66 Mini C...|This model featur...|           3975|   83.51|141.54|\n",
      "|   S18_1342|1937 Lincoln Berline|    Vintage Cars|        1:18|Motor City Art Cl...|Features opening ...|           8693|   60.62|102.74|\n",
      "|   S18_1367|1936 Mercedes-Ben...|    Vintage Cars|        1:18| Studio M Art Models|This 1:18 scale r...|           8635|   24.26| 53.91|\n",
      "+-----------+--------------------+----------------+------------+--------------------+--------------------+---------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark_sql_connect')\\\n",
    "                            .config(\"spark.jars\", \"./mysql-connector-j-8.0.32.jar\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .jdbc(\"jdbc:mysql://localhost:3306/classicmodels\", \"products\", \\\n",
    "          properties={\"user\": \"root\", \"driver\":\"com.mysql.cj.jdbc.Driver\"})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                null|           363|\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10102|2003-01-10|  2003-01-18| 2003-01-14|Shipped|                null|           181|\n",
      "|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                null|           121|\n",
      "|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                null|           141|\n",
      "|      10105|2003-02-11|  2003-02-21| 2003-02-12|Shipped|                null|           145|\n",
      "|      10106|2003-02-17|  2003-02-24| 2003-02-21|Shipped|                null|           278|\n",
      "|      10107|2003-02-24|  2003-03-03| 2003-02-26|Shipped|Difficult to nego...|           131|\n",
      "|      10108|2003-03-03|  2003-03-12| 2003-03-08|Shipped|                null|           385|\n",
      "|      10109|2003-03-10|  2003-03-19| 2003-03-11|Shipped|Customer requeste...|           486|\n",
      "|      10110|2003-03-18|  2003-03-24| 2003-03-20|Shipped|                null|           187|\n",
      "|      10111|2003-03-25|  2003-03-31| 2003-03-30|Shipped|                null|           129|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10113|2003-03-26|  2003-04-02| 2003-03-27|Shipped|                null|           124|\n",
      "|      10114|2003-04-01|  2003-04-07| 2003-04-02|Shipped|                null|           172|\n",
      "|      10115|2003-04-04|  2003-04-12| 2003-04-07|Shipped|                null|           424|\n",
      "|      10116|2003-04-11|  2003-04-19| 2003-04-13|Shipped|                null|           381|\n",
      "|      10117|2003-04-16|  2003-04-24| 2003-04-17|Shipped|                null|           148|\n",
      "|      10118|2003-04-21|  2003-04-29| 2003-04-26|Shipped|Customer has work...|           216|\n",
      "|      10119|2003-04-28|  2003-05-05| 2003-05-02|Shipped|                null|           382|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"jdbc\")\\\n",
    "             .option(\"driver\",\"com.mysql.cj.jdbc.Driver\")\\\n",
    "             .option(\"url\",\"jdbc:mysql://localhost:3306/classicmodels\")\\\n",
    "             .option(\"user\",\"root\")\\\n",
    "             .option(\"dbtable\",\"orders\")\\\n",
    "             .load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.0.32-cp39-cp39-macosx_12_0_arm64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.3,>=3.11.0\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.0.32 protobuf-3.20.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "#spark = SparkSession.builder.master(master).appName(appName).getOrCreate()\n",
    "conn = mysql.connector.connect(host=\"localhost\",\n",
    "                               user=\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('classicmodels',), ('information_schema',), ('mysql',), ('performance_schema',), ('sys',), ('test_db',)]\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"show databases;\")\n",
    "dbs = cur.fetchall()\n",
    "print(dbs)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b_/g3c_7hwd70j9k5t4lqfhswnh0000gn/T/ipykernel_2553/77378744.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pdf = pd.read_sql(query, con=conn)\n",
      "/Users/sae/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/sae/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|productCode|         productName|\n",
      "+-----------+--------------------+\n",
      "|   S10_1678|1969 Harley David...|\n",
      "|   S10_1949|1952 Alpine Renau...|\n",
      "|   S10_2016|1996 Moto Guzzi 1...|\n",
      "|   S10_4698|2003 Harley-David...|\n",
      "|   S10_4757| 1972 Alfa Romeo GTA|\n",
      "|   S10_4962|1962 LanciaA Delt...|\n",
      "|   S12_1099|   1968 Ford Mustang|\n",
      "|   S12_1108|   2001 Ferrari Enzo|\n",
      "|   S12_1666|      1958 Setra Bus|\n",
      "|   S12_2823|    2002 Suzuki XREO|\n",
      "|   S12_3148|  1969 Corvair Monza|\n",
      "|   S12_3380|  1968 Dodge Charger|\n",
      "|   S12_3891|    1969 Ford Falcon|\n",
      "|   S12_3990|1970 Plymouth Hem...|\n",
      "|   S12_4473|   1957 Chevy Pickup|\n",
      "|   S12_4675|  1969 Dodge Charger|\n",
      "|   S18_1097|1940 Ford Pickup ...|\n",
      "|   S18_1129|     1993 Mazda RX-7|\n",
      "|   S18_1342|1937 Lincoln Berline|\n",
      "|   S18_1367|1936 Mercedes-Ben...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "appName = \"PySpark+mysql.connector\"\n",
    "master = \"local\"\n",
    "\n",
    "spark = SparkSession.builder.master(master).appName(appName).getOrCreate()\n",
    "\n",
    "# Establish a connection\n",
    "conn = mysql.connector.connect(user='root', database='classicmodels',\n",
    "                               host=\"localhost\")\n",
    "cursor = conn.cursor()\n",
    "query = \"SELECT productCode, productName FROM products\"\n",
    "# Create a pandas dataframe\n",
    "pdf = pd.read_sql(query, con=conn)\n",
    "conn.close()\n",
    "\n",
    "# Convert Pandas dataframe to spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 23:10:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test SQL app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=\"classicmodels.orders\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # return number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderNumber: integer (nullable = true)\n",
      " |-- orderDate: date (nullable = true)\n",
      " |-- requiredDate: date (nullable = true)\n",
      " |-- shippedDate: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- customerNumber: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10320|2004-11-03|  2004-11-13| 2004-11-07|Shipped|                null|           144|\n",
      "|      10326|2004-11-09|  2004-11-16| 2004-11-10|Shipped|                null|           144|\n",
      "|      10334|2004-11-19|  2004-11-28|       null|On Hold|The outstaniding ...|           144|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read with custom query:\n",
    "query=\"(select * from orders where customerNumber = 144) as cust\"\n",
    "\n",
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=query).load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n",
      "|      10230|2004-03-15|  2004-03-24| 2004-03-20|Shipped|Customer very con...|           128|\n",
      "|      10300|2003-10-04|  2003-10-13| 2003-10-09|Shipped|                null|           128|\n",
      "|      10323|2004-11-05|  2004-11-12| 2004-11-09|Shipped|                null|           128|\n",
      "|      10112|2003-03-24|  2003-04-03| 2003-03-29|Shipped|Customer requeste...|           144|\n",
      "|      10320|2004-11-03|  2004-11-13| 2004-11-07|Shipped|                null|           144|\n",
      "|      10326|2004-11-09|  2004-11-16| 2004-11-10|Shipped|                null|           144|\n",
      "|      10334|2004-11-19|  2004-11-28|       null|On Hold|The outstaniding ...|           144|\n",
      "+-----------+----------+------------+-----------+-------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read with custom query:\n",
    "query=\"(select * from orders where customerNumber = 144 or customerNumber = 128) as cust\"\n",
    "\n",
    "df=spark.read.format(\"jdbc\").options(driver=\"com.mysql.cj.jdbc.Driver\",\\\n",
    "                                     user=\"root\",\\\n",
    "                                     url=\"jdbc:mysql://localhost:3306/classicmodels\",\\\n",
    "                                     dbtable=query \\\n",
    "                                    ).load()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation((select * from orders where customerNumber = 144 or customerNumber = 128) as cust) [numPartitions=1] [orderNumber#415,orderDate#416,requiredDate#417,shippedDate#418,status#419,comments#420,customerNumber#421] PushedFilters: [], ReadSchema: struct<orderNumber:int,orderDate:date,requiredDate:date,shippedDate:date,status:string,comments:s...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/10 06:09:48 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 971430 ms exceeds timeout 120000 ms\n",
      "23/02/10 06:09:48 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MLS=132842, Location='Arroyo Grande', Price=795000.0, Bedrooms=3, Bathrooms=3, Size=2371, Price SQ Ft=335.3, Status='Short Sale'),\n",
       " Row(MLS=134364, Location='Paso Robles', Price=399000.0, Bedrooms=4, Bathrooms=3, Size=2818, Price SQ Ft=141.59, Status='Short Sale'),\n",
       " Row(MLS=135141, Location='Paso Robles', Price=545000.0, Bedrooms=4, Bathrooms=3, Size=3032, Price SQ Ft=179.75, Status='Short Sale')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# withColumn:\n",
    "df = spark.read.load(\"spark_files/RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"PriceAfterService\", df.Price+ 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLS',\n",
       " 'Location',\n",
       " 'Price',\n",
       " 'Bedrooms',\n",
       " 'Bathrooms',\n",
       " 'Size',\n",
       " 'Price SQ Ft',\n",
       " 'Status',\n",
       " 'PriceAfterService']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "|   MLS|          Location|   Price|Bedrooms|Bathrooms|Size|Price SQ Ft|    Status|PriceAfterService|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "|132842|     Arroyo Grande|795000.0|       3|        3|2371|      335.3|Short Sale|         796000.0|\n",
      "|134364|       Paso Robles|399000.0|       4|        3|2818|     141.59|Short Sale|         400000.0|\n",
      "|135141|       Paso Robles|545000.0|       4|        3|3032|     179.75|Short Sale|         546000.0|\n",
      "|135712|         Morro Bay|909000.0|       4|        4|3540|     256.78|Short Sale|         910000.0|\n",
      "|136282|Santa Maria-Orcutt|109900.0|       3|        1|1249|      87.99|Short Sale|         110900.0|\n",
      "|136431|            Oceano|324900.0|       3|        3|1800|      180.5|Short Sale|         325900.0|\n",
      "|137036|Santa Maria-Orcutt|192900.0|       4|        2|1603|     120.34|Short Sale|         193900.0|\n",
      "|137090|Santa Maria-Orcutt|215000.0|       3|        2|1450|     148.28|Short Sale|         216000.0|\n",
      "+------+------------------+--------+--------+---------+----+-----------+----------+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+----------+\n",
      "|          Location|    Price|    Status|\n",
      "+------------------+---------+----------+\n",
      "|     Arroyo Grande| 795000.0|Short Sale|\n",
      "|       Paso Robles| 399000.0|Short Sale|\n",
      "|       Paso Robles| 545000.0|Short Sale|\n",
      "|         Morro Bay| 909000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 109900.0|Short Sale|\n",
      "|            Oceano| 324900.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 192900.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 215000.0|Short Sale|\n",
      "|         Morro Bay| 999000.0|Short Sale|\n",
      "|        Atascadero| 319000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 350000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 249000.0|Short Sale|\n",
      "|     Arroyo Grande| 299000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 235900.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 348000.0|Short Sale|\n",
      "|       Paso Robles| 314000.0|Short Sale|\n",
      "|        Los Alamos| 399000.0|Short Sale|\n",
      "|        San Miguel| 599000.0|Short Sale|\n",
      "|       Paso Robles| 299000.0|Short Sale|\n",
      "|   San Luis Obispo| 425000.0|Short Sale|\n",
      "|         Morro Bay|1100000.0|Short Sale|\n",
      "|           Cayucos|1500000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 110000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 200000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 134900.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 250000.0|Short Sale|\n",
      "|       Pismo Beach| 950000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 239950.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 170000.0|Short Sale|\n",
      "|Santa Maria-Orcutt| 285000.0|Short Sale|\n",
      "+------------------+---------+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SELECT select columns from df:\n",
    "df.select(\"Location\", \"Price\", \"Status\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|AMC Ambassador Br...|13.0|        8|       360.0|       175|  3821|        11.0|   73|    US|      25|NewYork|\n",
      "|  AMC Ambassador DPL|15.0|        8|       390.0|       190|  3850|         8.5|   70|    US|       2|     NJ|\n",
      "|  AMC Ambassador SST|17.0|        8|       304.0|       150|  3672|        11.5|   72|    US|       4| DALLAS|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----------+\n",
      "|Weight|Horsepower|\n",
      "+------+----------+\n",
      "|  3821|       175|\n",
      "|  3850|       190|\n",
      "+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf = spark.read.load(\"spark_files/cars.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "cardf.show(3)\n",
    "cardf[[\"Weight\", \"Horsepower\"]].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+------------------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|          hw_ratio|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+------------------+\n",
      "|AMC Ambassador Br...|13.0|        8|       360.0|       175|  3821|        11.0|   73|    US|      25|NewYork|21.834285714285713|\n",
      "|  AMC Ambassador DPL|15.0|        8|       390.0|       190|  3850|         8.5|   70|    US|       2|     NJ|20.263157894736842|\n",
      "|  AMC Ambassador SST|17.0|        8|       304.0|       150|  3672|        11.5|   72|    US|       4| DALLAS|             24.48|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select with withcolumn\n",
    "cardf.withColumn(\"hw_ratio\", cardf[\"Weight\"]/cardf[\"Horsepower\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean,avg,max,min\n",
    "from pyspark.sql.functions import round as round_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|round(Acceleration, 0)|\n",
      "+----------------------+\n",
      "|                  11.0|\n",
      "|                   9.0|\n",
      "+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.select(round_(\"Acceleration\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car: string (nullable = true)\n",
      " |-- MPG: double (nullable = true)\n",
      " |-- Cylinders: integer (nullable = true)\n",
      " |-- Displacement: double (nullable = true)\n",
      " |-- Horsepower: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Acceleration: double (nullable = true)\n",
      " |-- Model: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|max(quantity)|min(quantity)|\n",
      "+-------------+-------------+\n",
      "|        75275|            2|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.select(max(\"quantity\"),min(\"quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df = spark.read.csv(\"/Users/sae/Downloads/stack-overflow-developer-survey-2022/survey_results_public.csv\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
